Search.setIndex({"docnames": ["docs_main/consts_values", "docs_main/io_utilities", "docs_main/main", "docs_main/model_utilities", "docs_main/modules", "docs_main/preprocessing_flow", "docs_main/save_model_bento_ml", "docs_main/text_preprocessing_utilities", "docs_main/unit_test_model_utilities", "docs_main/unit_test_text_preprocessing_utilities", "docs_model_classes/CountVectorizerFE", "docs_model_classes/Doc2VecFE", "docs_model_classes/FeaturesExtractor", "docs_model_classes/HashingVectorizerFE", "docs_model_classes/ModelManager", "docs_model_classes/ModelWorker", "docs_model_classes/StaticClassifier", "docs_model_classes/TfidfVectorizerFE", "docs_model_classes/modules", "index"], "filenames": ["docs_main\\consts_values.rst", "docs_main\\io_utilities.rst", "docs_main\\main.rst", "docs_main\\model_utilities.rst", "docs_main\\modules.rst", "docs_main\\preprocessing_flow.rst", "docs_main\\save_model_bento_ml.rst", "docs_main\\text_preprocessing_utilities.rst", "docs_main\\unit_test_model_utilities.rst", "docs_main\\unit_test_text_preprocessing_utilities.rst", "docs_model_classes\\CountVectorizerFE.rst", "docs_model_classes\\Doc2VecFE.rst", "docs_model_classes\\FeaturesExtractor.rst", "docs_model_classes\\HashingVectorizerFE.rst", "docs_model_classes\\ModelManager.rst", "docs_model_classes\\ModelWorker.rst", "docs_model_classes\\StaticClassifier.rst", "docs_model_classes\\TfidfVectorizerFE.rst", "docs_model_classes\\modules.rst", "index.rst"], "titles": ["consts_values module", "io_utilities module", "main module", "model_utilities module", "main", "preprocessing_flow module", "save_model_bento_ml module", "text_preprocessing_utilities module", "unit_test_model_utilities module", "unit_test_text_preprocessing_utilities module", "CountVectorizerFE module", "Doc2VecFE module", "FeaturesExtractor module", "HashingVectorizerFE module", "ModelManager module", "ModelWorker module", "StaticClassifier module", "TfidfVectorizerFE module", "model_classes", "Welcome to MLO-DCM\u2019s documentation!"], "terms": {"src": [1, 3, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "main": [1, 3, 5, 7, 8, 9, 15, 19], "export_as_binary_obj": [1, 4], "obj": 1, "output_file_path": [1, 3], "sourc": [1, 3, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "function": [1, 3, 5, 7, 8, 9, 14, 15], "serial": 1, "given": [1, 3, 5, 7, 9, 15, 16], "object": [1, 7, 12, 14, 15, 16], "save": [1, 3, 5, 14], "binari": [1, 14], "file": [1, 5, 7, 14, 15], "param": [1, 3, 5, 7, 10, 11, 12, 13, 14, 15, 16, 17], "an": [1, 7, 9], "can": [1, 7, 14], "list": [1, 5, 7, 9, 11, 14, 16], "scaler": 1, "classifi": [1, 14, 15, 16], "path": [1, 3, 5, 14, 15], "where": [1, 3, 5, 10, 13, 14, 16, 17], "json": 1, "return": [1, 3, 5, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17], "none": [1, 3, 5, 7, 12, 13, 14], "import_binary_object": [1, 4], "input_file_path": 1, "us": [1, 3, 5, 7, 12, 14, 15, 16], "import": [1, 15], "pickl": 1, "need": 1, "unseri": 1, "python": [1, 3, 5, 7, 10, 11, 13, 14, 15, 16, 17], "case": [1, 5, 7, 9], "error": 1, "dure": 1, "read": [1, 5, 14, 15], "read_raw_data": [1, 4], "main_directory_path": 1, "creat": [1, 5, 7, 9, 14], "datafram": [1, 3, 5, 8], "data": [1, 3, 5, 8, 10, 11, 12, 13, 14, 15, 16, 17], "store": [1, 3, 5], "The": [1, 3, 11, 12, 14], "directori": [1, 5, 14], "ha": [1, 3, 7, 9, 12], "structur": [1, 7], "like": [1, 7], "main_directori": 1, "subdirectory_category1": 1, "subdirectory_category2": 1, "subdirectory_category3": 1, "3": 1, "column": 1, "content": [1, 5, 10, 11, 13, 16, 17], "type": [1, 3, 5, 7, 9], "rtype": [1, 3, 5, 7, 10, 11, 13, 14, 15, 16, 17], "panda": [1, 3, 5, 11, 14], "core": [1, 3, 5], "frame": [1, 3, 5, 11, 14], "read_txt_fil": [1, 4], "file_path": [1, 5, 15], "from": [1, 3, 5, 7, 9, 12, 14, 15, 16], "paramet": [1, 5, 7, 10, 13, 16, 17], "target": [1, 3, 14], "built": [1, 7, 11], "string": [1, 5, 7, 9, 10, 11, 13, 14, 15, 16, 17], "save_dict_to_json_fil": [1, 4], "dictionari": [1, 3, 7, 8, 10, 12, 13, 14, 15, 16, 17], "contain": [1, 3, 7, 9, 14], "pair": [1, 3, 10, 12, 14, 15, 17], "kei": [1, 3, 7, 10, 13, 16, 17], "str_valu": 1, "": [1, 3, 7, 9], "int_valu": 1, "build_data_dictionari": [3, 4], "x_train": 3, "x_test": 3, "y_train": 3, "y_test": 3, "construct": [3, 7, 8], "train": [3, 8, 14, 15, 16], "test": [3, 7, 8, 9, 14, 16], "input": [3, 5, 7, 9, 10, 11, 13, 15, 16, 17], "variabl": [3, 10, 11, 13, 14, 16, 17], "valu": [3, 5, 7, 9, 10, 11, 12, 13, 15, 16, 17], "set": [3, 8, 11, 14], "build": [3, 5, 7, 10, 11, 13, 14, 15, 16, 17], "get_model_evaluation_metr": [3, 4], "confusion_matrix": 3, "comput": [3, 8, 15], "manual": [3, 8], "differ": [3, 8, 9, 16], "metric": [3, 8, 14, 15, 16], "e": [3, 7, 12, 16], "g": [3, 7, 16], "accuraci": [3, 15], "precis": 3, "recal": 3, "specif": [3, 7, 14, 15], "f1": 3, "score": [3, 15], "etc": [3, 16], "onli": [3, 7, 14], "confus": [3, 15, 16], "matrix": [3, 10, 13, 16, 17], "calcul": [3, 16], "model": [3, 5, 7, 11, 14, 15, 16], "mean": 3, "obtain": [3, 14], "shuffle_datafram": [3, 4], "df": [3, 5, 11], "no_of_tim": 3, "1": 3, "shuffl": [3, 8], "x": [3, 8, 12, 14], "time": [3, 8], "row": [3, 11], "number": [3, 5, 7, 9, 15], "split_model_data": [3, 4], "x_data": [3, 14], "y_data": [3, 14], "test_size_valu": 3, "0": 3, "25": 3, "random_state_v": 3, "split": [3, 7, 8, 9, 14], "stratifi": 3, "y": 3, "fashion": 3, "thi": [3, 7, 11, 12, 14, 15, 16], "class": [3, 5, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "label": [3, 5, 15, 16], "becaus": 3, "we": [3, 7, 11, 12, 14, 16], "have": [3, 7, 16], "mani": 3, "scope": 3, "being": 3, "uniform": 3, "distribut": 3, "respect": 3, "i": [3, 5, 7, 9, 11, 12, 14, 16], "properli": 3, "ar": [3, 9, 10, 11, 12, 13, 14, 16, 17], "select": 3, "ob": 3, "9": 3, "distinct": 3, "tenth": 3, "proport": 3, "dataset": [3, 14], "repres": [3, 7, 12, 14, 15, 16], "reproduc": 3, "certain": [3, 7], "result": [3, 14, 15, 16], "determinist": 3, "vocabulary_dict_to_json": [3, 4], "provid": [3, 12, 14, 15], "str": [3, 5, 7, 9, 11, 15], "int": 3, "user": [3, 16], "want": [3, 7, 11, 14], "consts_valu": [4, 19], "modul": [4, 18, 19], "io_util": [4, 19], "model_util": [4, 19], "preprocessing_flow": [4, 19], "apply_custom_tokenizer_it": [4, 5], "custom_token": [4, 5], "get_nlp_model": [4, 5], "preprocess_fil": [4, 5], "process_df": [4, 5], "read_preprocess_and_export": [4, 5], "save_model_bento_ml": [4, 19], "text_preprocessing_util": [4, 19], "get_lowercase_words_from_str": [4, 7], "get_rare_token": [4, 7], "get_spacy_tokens_from_raw_text": [4, 7], "get_str_stopword": [4, 7], "get_str_tokens_freq": [4, 7], "get_str_tokens_freq_for_list": [4, 7], "handle_rare_str_token": [4, 7], "is_6digits_d": [4, 7], "is_str_fract": [4, 7], "is_str_numer": [4, 7, 9], "is_str_valid_d": [4, 7], "is_valid_resourc": [4, 7], "is_valid_url": [4, 7], "lemmatize_spacy_token": [4, 7], "remove_excessive_spac": [4, 7], "remove_spacy_punctu": [4, 7], "remove_str_tokens_len_less_than_threshold": [4, 7], "spacy_tokens_po": [4, 7], "spacy_tokens_to_str_token": [4, 7], "split_and_gather_str_tokens_by_separ": [4, 7], "str_6digits_dates_to_date_tag": [4, 7], "str_currency_to_spoken_word": [4, 7], "str_dates_to_date_tag": [4, 7], "str_emails_to_email_tag": [4, 7], "str_fraction_to_spoken_word": [4, 7], "str_initial_case_to_tag": [4, 7], "str_number_with_separators_to_integer_numb": [4, 7], "str_numeric_values_to_spoken_word": [4, 7], "str_ordinal_numbers_to_spoken_word": [4, 7], "str_remove_common_char": [4, 7], "str_remove_junk_spac": [4, 7], "str_tokens_numbers_with_separators_to_spoken_word": [4, 7], "str_tokens_remove_stopword": [4, 7], "str_tokens_replace_symbol_with_tag": [4, 7], "str_tokens_to_spacy_token": [4, 7], "str_tokens_to_str": [4, 7], "str_urls_to_url_tag": [4, 7], "str_years_to_spoken_word": [4, 7], "to_lowercas": [4, 7], "unit_test_model_util": [4, 19], "unittest": [4, 8, 9], "test_build_data_dictionari": [4, 8], "test_get_model_evaluation_metr": [4, 8], "test_shuffle_datafram": [4, 8], "test_split_model_data": [4, 8], "test_vocabulary_dict_to_json": [4, 8], "unit_test_text_preprocessing_util": [4, 19], "test_get_lowercase_words_from_str": [4, 9], "test_get_rare_token": [4, 9], "test_get_spacy_tokens_from_raw_text": [4, 9], "test_get_str_tokens_freq": [4, 9], "test_get_str_tokens_freq_for_list": [4, 9], "test_handle_rare_str_token": [4, 9], "test_is_6digits_d": [4, 9], "test_is_str_fract": [4, 9], "test_is_str_numer": [4, 9], "test_is_str_valid_d": [4, 9], "test_is_valid_resourc": [4, 9], "test_is_valid_url": [4, 9], "test_lemmatize_spacy_token": [4, 9], "test_remove_excessive_spac": [4, 9], "test_remove_spacy_punctu": [4, 9], "test_remove_str_tokens_len_less_than_threshold": [4, 9], "test_spacy_tokens_po": [4, 9], "test_spacy_tokens_to_str_token": [4, 9], "test_split_and_gather_str_tokens_by_separ": [4, 9], "test_str_6digits_dates_to_date_tag": [4, 9], "test_str_currency_to_spoken_word": [4, 9], "test_str_dates_to_date_tag": [4, 9], "test_str_emails_to_email_tag": [4, 9], "test_str_fraction_to_spoken_word": [4, 9], "test_str_initial_case_to_tag": [4, 9], "test_str_number_with_separators_to_integer_numb": [4, 9], "test_str_numeric_values_to_spoken_word": [4, 9], "test_str_ordinal_numbers_to_spoken_word": [4, 9], "test_str_remove_common_char": [4, 9], "test_str_remove_junk_spac": [4, 9], "test_str_tokens_numbers_with_separators_to_spoken_word": [4, 9], "test_str_tokens_remove_stopword": [4, 9], "test_str_tokens_replace_quote_with_tag": [4, 9], "test_str_tokens_to_spacy_token": [4, 9], "test_str_tokens_to_str": [4, 9], "test_str_urls_to_url_tag": [4, 9], "test_str_years_to_spoken_word": [4, 9], "test_to_lowercas": [4, 9], "raw_text": [5, 15], "nlp_model": [5, 7], "iter": 5, "2": [5, 15], "appli": [5, 7], "token": [5, 7, 9, 12, 15], "over": 5, "raw": [5, 7, 9, 15], "text": [5, 7, 9, 12, 14, 15], "similar": 5, "usag": 5, "epoch": 5, "deep": [5, 16], "learn": [5, 16], "process": [5, 14], "made": 5, "all": [5, 7, 9, 14, 15], "necessari": 5, "transform": [5, 7, 9, 10, 11, 12, 13, 14, 17], "includ": [5, 7, 9, 14, 15], "unprocess": 5, "probabl": [5, 15, 16], "spaci": [5, 7, 9], "lang": [5, 7], "en": 5, "english": 5, "preprocess": [5, 7, 14, 15, 16], "instanc": [5, 7, 12, 14], "nlp": [5, 7], "declar": 5, "requir": [5, 16], "its": 5, "compos": 5, "present": 5, "separ": [5, 7, 9], "preprocessing_iter": 5, "specifi": [5, 9], "mayb": 5, "other": [5, 7, 14, 16], "col": 5, "singl": [5, 7, 11], "directory_path": [5, 14], "output_file_nam": 5, "subdirectori": 5, "func": 5, "csv": [5, 14], "name": [5, 10, 11, 13, 14, 16, 17], "val": 7, "extract": [7, 9], "lowercas": [7, 9], "word": [7, 9, 10, 13, 17], "one": [7, 14], "hous": 7, "without": 7, "charact": 7, "lower": [7, 9], "dict_of_freq": 7, "threshold": [7, 9], "get": [7, 9, 12, 16], "frequenc": [7, 9], "smaller": [7, 9], "than": [7, 9], "lists_of_token": 7, "comparison": 7, "new": [7, 10, 11, 12, 13, 16, 17], "just": [7, 14], "convert": [7, 12, 14, 15], "nativ": 7, "stopword": [7, 9], "exclud": 7, "consid": [7, 14, 15, 16], "replace_with": 7, "unk": 7, "filter": 7, "elimin": [7, 9], "rare": [7, 9], "replac": [7, 9], "remov": [7, 9], "els": 7, "item": 7, "verifi": 7, "calendar": [7, 9], "date": [7, 9], "6": [7, 9], "digit": [7, 9], "format": [7, 14, 15], "14": 7, "05": 7, "93": 7, "refer": [7, 16], "year": [7, 9], "bool": 7, "true": 7, "fals": [7, 14], "otherwis": 7, "valid": [7, 9], "fraction": [7, 9], "numer": [7, 9, 11, 12, 14, 15], "check": [7, 9], "url": [7, 9], "small": [7, 9], "notat": [7, 9], "kind": 7, "resourc": 7, "am": 7, "arc": 7, "nasa": 7, "gov": 7, "uri": 7, "lemmat": 7, "everi": [7, 14, 16], "element": [7, 11], "initi": [7, 11], "base": [7, 8, 9, 10, 11, 12, 13, 15, 16, 17], "form": [7, 9], "excess": 7, "white": 7, "space": [7, 9], "begin": 7, "end": 7, "punctuat": [7, 9], "threshold_valu": 7, "length": [7, 9], "minim": 7, "modifi": 7, "tupl": [7, 9, 14, 15], "posit": [7, 9, 11, 12], "equival": 7, "entiti": [7, 12], "tech": 7, "media": 7, "which": 7, "becom": 7, "after": [7, 12], "c_date": [7, 9], "tag": [7, 9], "apparit": 7, "currenc": [7, 9], "reconstruct": 7, "instead": [7, 12], "email": [7, 9], "address": 7, "constant": 7, "produc": 7, "some": [7, 12], "chain": 7, "half": 7, "letter": 7, "f": 7, "appear": [7, 9], "comma": [7, 9], "integ": [7, 9, 15], "10": 7, "500": 7, "205": 7, "10500205": 7, "ordin": [7, 9], "common": [7, 9], "char": [7, 9], "junk": [7, 9], "spoken": [7, 9], "append": 7, "them": [7, 15, 16], "ten": 7, "million": 7, "five": 7, "hundr": 7, "thousand": 7, "two": 7, "defin": 7, "also": 7, "symbol": [7, 9], "mention": 7, "cat": 7, "quot": 7, "what": 7, "methodnam": [8, 9], "runtest": [8, 9], "testcas": [8, 9], "unit": [8, 9], "multipl": 9, "mix": 9, "upper": 9, "thst": 9, "invalid": 9, "when": [9, 14], "non": 9, "lemat": 9, "see": 9, "null": 9, "model_class": [10, 11, 12, 13, 14, 15, 16, 17, 19], "featuresextractor": [10, 11, 13, 14, 17, 18, 19], "get_extractor_param": [10, 11, 12, 13, 17, 18], "getter": [10, 11, 12, 13, 16, 17], "copi": [10, 11, 12, 13, 16, 17], "get_vocabulari": [10, 12, 13, 17, 18], "method": [10, 11, 12, 13, 14, 16, 17], "uniqu": [10, 13, 17], "featur": [10, 11, 12, 13, 14, 15, 16, 17], "correspond": [10, 13, 16, 17], "indic": [10, 13, 17], "set_extractor_param": [10, 11, 12, 13, 17, 18], "new_param": [10, 11, 12, 13, 16, 17], "setter": [10, 12, 13, 16, 17], "short_str": [10, 11, 13, 16, 17, 18], "transform_data": [10, 11, 12, 13, 17, 18], "For": 11, "doc2vec": 11, "extractor": [11, 12, 14, 15], "cannot": 11, "wai": [11, 16], "If": 11, "you": 11, "chang": 11, "mandatori": 11, "retrain": 11, "again": 11, "so": 11, "would": 11, "seri": [11, 14], "scale": 11, "capabl": 12, "later": [12, 14], "classif": [12, 14, 15], "receiv": [12, 14, 15], "sklearn": [12, 16], "act": 12, "abstract": 12, "directli": 12, "deriv": 12, "get_data": [12, 18], "pure": 12, "virtual": [12, 13], "vocabulari": 12, "set_data": [12, 18], "new_data": 12, "pass": [12, 16], "assum": [12, 14, 16], "been": 12, "alreadi": [12, 16], "fit": [12, 14, 16], "work": [14, 15, 16], "manag": 14, "along": 14, "step": 14, "flow": 14, "follow": 14, "our": 14, "origin": 14, "concret": 14, "reus": 14, "stage": 14, "cross": 14, "product": 14, "logic": 14, "features_extractor": 14, "evalu": [14, 15], "observ": 14, "onc": 14, "per": 14, "ani": 14, "thu": 14, "cl": 14, "k": 14, "locat": [14, 15], "aim": 14, "persist": 14, "predict": [14, 15, 16, 18], "request": 14, "build_classifi": [14, 18], "initialis": 14, "build_features_extractor": [14, 18], "get_classifier_to_extractor_str": [14, 18], "classifier_nam": [14, 15], "features_extractor_nam": 14, "manager_execut": [14, 18], "input_data_path": 14, "output_objects_path": 14, "save_model_obj": 14, "aggreg": 14, "oper": 14, "root": [14, 15], "pf": 14, "should": 14, "option": 14, "model_fit_train_predict": [14, 18], "save_model_object": 14, "compon": 14, "run": 14, "independ": 14, "staticclassifi": [14, 18, 19], "amd": 14, "reverse_classifier_to_extractor_str": [14, 18], "compound_nam": 14, "compound": 14, "revers": 14, "engin": 14, "save_model_compon": [14, 18], "object_nam": 14, "worker": 15, "servant": 15, "It": 15, "predefin": 15, "vote": 15, "system": 15, "evaluate_classifi": [15, 18], "perform": 15, "matric": 15, "d": 15, "dict": 15, "associ": 15, "import_classifi": [15, 18], "extractor_nam": 15, "import_features_extractor": [15, 18], "import_model_object": [15, 18], "perform_predict": [15, 18], "processed_text": 15, "insid": 15, "addit": 15, "inform": 15, "top": 15, "predict_input_from_fil": [15, 18], "preprocess_input": [15, 18], "join": 15, "voting_system": [15, 18], "dict_with_predict": 15, "n_highest_prob": 15, "classifier_extractor_nam": 15, "highest": 15, "occurr": 15, "occur": 15, "worker_execut": [15, 18], "call": 15, "static": 16, "context": 16, "classic": 16, "solver": 16, "randomforestclassifi": 16, "xgboostclassifi": 16, "svm": 16, "heavili": 16, "neural": 16, "network": 16, "usual": 16, "complex": 16, "architectur": 16, "thing": 16, "must": 16, "wrapper": 16, "packag": 16, "incorpor": 16, "implement": 16, "allow": 16, "proper": 16, "model_classifi": 16, "fit_train_evalu": [16, 18], "dict_data": 16, "get_confusion_matrix": [16, 18], "get_model_class": [16, 18], "get_model_param": [16, 18], "data_point": 16, "predict_prob": [16, 18], "point": 16, "set_model_param": [16, 18], "map": 16, "n_estim": 16, "200": 16, "countvectorizerf": [18, 19], "doc2vecf": [18, 19], "hashingvectorizerf": [18, 19], "modelmanag": [18, 19], "modelwork": [18, 19], "tfidfvectorizerf": [18, 19], "index": 19, "search": 19, "page": 19}, "objects": {"src.main": [[0, 0, 0, "-", "consts_values"], [1, 0, 0, "-", "io_utilities"], [2, 0, 0, "-", "main"], [3, 0, 0, "-", "model_utilities"], [5, 0, 0, "-", "preprocessing_flow"], [6, 0, 0, "-", "save_model_bento_ml"], [7, 0, 0, "-", "text_preprocessing_utilities"], [8, 0, 0, "-", "unit_test_model_utilities"], [9, 0, 0, "-", "unit_test_text_preprocessing_utilities"]], "src.main.io_utilities": [[1, 1, 1, "", "export_as_binary_obj"], [1, 1, 1, "", "import_binary_object"], [1, 1, 1, "", "read_raw_data"], [1, 1, 1, "", "read_txt_file"], [1, 1, 1, "", "save_dict_to_json_file"]], "src.main.model_utilities": [[3, 1, 1, "", "build_data_dictionary"], [3, 1, 1, "", "get_model_evaluation_metrics"], [3, 1, 1, "", "shuffle_dataframe"], [3, 1, 1, "", "split_model_data"], [3, 1, 1, "", "vocabulary_dict_to_json"]], "src.main.preprocessing_flow": [[5, 1, 1, "", "apply_custom_tokenizer_iteratively"], [5, 1, 1, "", "custom_tokenizer"], [5, 1, 1, "", "get_nlp_model"], [5, 1, 1, "", "preprocess_file"], [5, 1, 1, "", "process_df"], [5, 1, 1, "", "read_preprocess_and_export"]], "src.main.text_preprocessing_utilities": [[7, 1, 1, "", "get_lowercase_words_from_str"], [7, 1, 1, "", "get_rare_tokens"], [7, 1, 1, "", "get_spacy_tokens_from_raw_text"], [7, 1, 1, "", "get_str_stopwords"], [7, 1, 1, "", "get_str_tokens_freq"], [7, 1, 1, "", "get_str_tokens_freq_for_lists"], [7, 1, 1, "", "handle_rare_str_tokens"], [7, 1, 1, "", "is_6digits_date"], [7, 1, 1, "", "is_str_fraction"], [7, 1, 1, "", "is_str_numeric"], [7, 1, 1, "", "is_str_valid_date"], [7, 1, 1, "", "is_valid_resource"], [7, 1, 1, "", "is_valid_url"], [7, 1, 1, "", "lemmatize_spacy_tokens"], [7, 1, 1, "", "remove_excessive_space"], [7, 1, 1, "", "remove_spacy_punctuations"], [7, 1, 1, "", "remove_str_tokens_len_less_than_threshold"], [7, 1, 1, "", "spacy_tokens_pos"], [7, 1, 1, "", "spacy_tokens_to_str_tokens"], [7, 1, 1, "", "split_and_gather_str_tokens_by_separator"], [7, 1, 1, "", "str_6digits_dates_to_date_tag"], [7, 1, 1, "", "str_currency_to_spoken_words"], [7, 1, 1, "", "str_dates_to_date_tag"], [7, 1, 1, "", "str_emails_to_email_tag"], [7, 1, 1, "", "str_fraction_to_spoken_words"], [7, 1, 1, "", "str_initial_case_to_tag"], [7, 1, 1, "", "str_number_with_separators_to_integer_number"], [7, 1, 1, "", "str_numeric_values_to_spoken_words"], [7, 1, 1, "", "str_ordinal_numbers_to_spoken_words"], [7, 1, 1, "", "str_remove_common_chars"], [7, 1, 1, "", "str_remove_junk_spaces"], [7, 1, 1, "", "str_tokens_numbers_with_separators_to_spoken_words"], [7, 1, 1, "", "str_tokens_remove_stopwords"], [7, 1, 1, "", "str_tokens_replace_symbol_with_tag"], [7, 1, 1, "", "str_tokens_to_spacy_tokens"], [7, 1, 1, "", "str_tokens_to_str"], [7, 1, 1, "", "str_urls_to_url_tag"], [7, 1, 1, "", "str_years_to_spoken_words"], [7, 1, 1, "", "to_lowercase"]], "src.main.unit_test_model_utilities": [[8, 2, 1, "", "UnitTests"]], "src.main.unit_test_model_utilities.UnitTests": [[8, 3, 1, "", "test_build_data_dictionary"], [8, 3, 1, "", "test_get_model_evaluation_metrics"], [8, 3, 1, "", "test_shuffle_dataframe"], [8, 3, 1, "", "test_split_model_data"], [8, 3, 1, "", "test_vocabulary_dict_to_json"]], "src.main.unit_test_text_preprocessing_utilities": [[9, 2, 1, "", "UnitTests"]], "src.main.unit_test_text_preprocessing_utilities.UnitTests": [[9, 3, 1, "", "test_get_lowercase_words_from_str"], [9, 3, 1, "", "test_get_rare_tokens"], [9, 3, 1, "", "test_get_spacy_tokens_from_raw_text"], [9, 3, 1, "", "test_get_str_tokens_freq"], [9, 3, 1, "", "test_get_str_tokens_freq_for_lists"], [9, 3, 1, "", "test_handle_rare_str_tokens"], [9, 3, 1, "", "test_is_6digits_date"], [9, 3, 1, "", "test_is_str_fraction"], [9, 3, 1, "", "test_is_str_numeric"], [9, 3, 1, "", "test_is_str_valid_date"], [9, 3, 1, "", "test_is_valid_resource"], [9, 3, 1, "", "test_is_valid_url"], [9, 3, 1, "", "test_lemmatize_spacy_tokens"], [9, 3, 1, "", "test_remove_excessive_space"], [9, 3, 1, "", "test_remove_spacy_punctuations"], [9, 3, 1, "", "test_remove_str_tokens_len_less_than_threshold"], [9, 3, 1, "", "test_spacy_tokens_pos"], [9, 3, 1, "", "test_spacy_tokens_to_str_tokens"], [9, 3, 1, "", "test_split_and_gather_str_tokens_by_separator"], [9, 3, 1, "", "test_str_6digits_dates_to_date_tag"], [9, 3, 1, "", "test_str_currency_to_spoken_words"], [9, 3, 1, "", "test_str_dates_to_date_tag"], [9, 3, 1, "", "test_str_emails_to_email_tag"], [9, 3, 1, "", "test_str_fraction_to_spoken_words"], [9, 3, 1, "", "test_str_initial_case_to_tag"], [9, 3, 1, "", "test_str_number_with_separators_to_integer_number"], [9, 3, 1, "", "test_str_numeric_values_to_spoken_words"], [9, 3, 1, "", "test_str_ordinal_numbers_to_spoken_words"], [9, 3, 1, "", "test_str_remove_common_chars"], [9, 3, 1, "", "test_str_remove_junk_spaces"], [9, 3, 1, "", "test_str_tokens_numbers_with_separators_to_spoken_words"], [9, 3, 1, "", "test_str_tokens_remove_stopwords"], [9, 3, 1, "", "test_str_tokens_replace_quote_with_tag"], [9, 3, 1, "", "test_str_tokens_to_spacy_tokens"], [9, 3, 1, "", "test_str_tokens_to_str"], [9, 3, 1, "", "test_str_urls_to_url_tag"], [9, 3, 1, "", "test_str_years_to_spoken_words"], [9, 3, 1, "", "test_to_lowercase"]], "src.model_classes": [[10, 0, 0, "-", "CountVectorizerFE"], [11, 0, 0, "-", "Doc2VecFE"], [12, 0, 0, "-", "FeaturesExtractor"], [13, 0, 0, "-", "HashingVectorizerFE"], [14, 0, 0, "-", "ModelManager"], [15, 0, 0, "-", "ModelWorker"], [16, 0, 0, "-", "StaticClassifier"], [17, 0, 0, "-", "TfidfVectorizerFE"]], "src.model_classes.CountVectorizerFE": [[10, 2, 1, "", "CountVectorizerFE"]], "src.model_classes.CountVectorizerFE.CountVectorizerFE": [[10, 3, 1, "", "get_extractor_params"], [10, 3, 1, "", "get_vocabulary"], [10, 3, 1, "", "set_extractor_params"], [10, 3, 1, "", "short_str"], [10, 3, 1, "", "transform_data"]], "src.model_classes.Doc2VecFE": [[11, 2, 1, "", "Doc2VecFE"]], "src.model_classes.Doc2VecFE.Doc2VecFE": [[11, 3, 1, "", "get_extractor_params"], [11, 3, 1, "", "set_extractor_params"], [11, 3, 1, "", "short_str"], [11, 3, 1, "", "transform_data"]], "src.model_classes.FeaturesExtractor": [[12, 2, 1, "", "FeaturesExtractor"]], "src.model_classes.FeaturesExtractor.FeaturesExtractor": [[12, 3, 1, "", "get_data"], [12, 3, 1, "", "get_extractor_params"], [12, 3, 1, "", "get_vocabulary"], [12, 3, 1, "", "set_data"], [12, 3, 1, "", "set_extractor_params"], [12, 3, 1, "", "transform_data"]], "src.model_classes.HashingVectorizerFE": [[13, 2, 1, "", "HashingVectorizerFE"]], "src.model_classes.HashingVectorizerFE.HashingVectorizerFE": [[13, 3, 1, "", "get_extractor_params"], [13, 3, 1, "", "get_vocabulary"], [13, 3, 1, "", "set_extractor_params"], [13, 3, 1, "", "short_str"], [13, 3, 1, "", "transform_data"]], "src.model_classes.ModelManager": [[14, 1, 1, "", "build_classifiers"], [14, 1, 1, "", "build_features_extractors"], [14, 1, 1, "", "get_classifier_to_extractor_str"], [14, 1, 1, "", "manager_execute"], [14, 1, 1, "", "model_fit_train_predict"], [14, 1, 1, "", "reverse_classifier_to_extractor_str"], [14, 1, 1, "", "save_model_component"]], "src.model_classes.ModelWorker": [[15, 1, 1, "", "evaluate_classifiers"], [15, 1, 1, "", "import_classifiers"], [15, 1, 1, "", "import_features_extractors"], [15, 1, 1, "", "import_model_objects"], [15, 1, 1, "", "perform_prediction"], [15, 1, 1, "", "predict_input_from_file"], [15, 1, 1, "", "preprocess_input"], [15, 1, 1, "", "voting_system"], [15, 1, 1, "", "worker_execute"]], "src.model_classes.StaticClassifier": [[16, 2, 1, "", "StaticClassifier"]], "src.model_classes.StaticClassifier.StaticClassifier": [[16, 3, 1, "", "fit_train_evaluate"], [16, 3, 1, "", "get_confusion_matrix"], [16, 3, 1, "", "get_model_classes"], [16, 3, 1, "", "get_model_params"], [16, 3, 1, "", "predict"], [16, 3, 1, "", "predict_probabilities"], [16, 3, 1, "", "set_model_params"], [16, 3, 1, "", "short_str"]], "src.model_classes.TfidfVectorizerFE": [[17, 2, 1, "", "TfidfVectorizerFE"]], "src.model_classes.TfidfVectorizerFE.TfidfVectorizerFE": [[17, 3, 1, "", "get_extractor_params"], [17, 3, 1, "", "get_vocabulary"], [17, 3, 1, "", "set_extractor_params"], [17, 3, 1, "", "short_str"], [17, 3, 1, "", "transform_data"]]}, "objtypes": {"0": "py:module", "1": "py:function", "2": "py:class", "3": "py:method"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "function", "Python function"], "2": ["py", "class", "Python class"], "3": ["py", "method", "Python method"]}, "titleterms": {"consts_valu": 0, "modul": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "io_util": 1, "main": [2, 4], "model_util": 3, "preprocessing_flow": 5, "save_model_bento_ml": 6, "text_preprocessing_util": 7, "unit_test_model_util": 8, "unit_test_text_preprocessing_util": 9, "countvectorizerf": 10, "doc2vecf": 11, "featuresextractor": 12, "hashingvectorizerf": 13, "modelmanag": 14, "modelwork": 15, "staticclassifi": 16, "tfidfvectorizerf": 17, "model_class": 18, "welcom": 19, "mlo": 19, "dcm": 19, "": 19, "document": 19, "content": 19, "indic": 19, "tabl": 19}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.viewcode": 1, "sphinx.ext.todo": 2, "sphinx": 58}, "alltitles": {"consts_values module": [[0, "module-src.main.consts_values"]], "io_utilities module": [[1, "module-src.main.io_utilities"]], "main module": [[2, "module-src.main.main"]], "model_utilities module": [[3, "module-src.main.model_utilities"]], "main": [[4, "main"]], "preprocessing_flow module": [[5, "module-src.main.preprocessing_flow"]], "save_model_bento_ml module": [[6, "module-src.main.save_model_bento_ml"]], "text_preprocessing_utilities module": [[7, "module-src.main.text_preprocessing_utilities"]], "unit_test_model_utilities module": [[8, "module-src.main.unit_test_model_utilities"]], "unit_test_text_preprocessing_utilities module": [[9, "module-src.main.unit_test_text_preprocessing_utilities"]], "CountVectorizerFE module": [[10, "module-src.model_classes.CountVectorizerFE"]], "Doc2VecFE module": [[11, "module-src.model_classes.Doc2VecFE"]], "FeaturesExtractor module": [[12, "module-src.model_classes.FeaturesExtractor"]], "HashingVectorizerFE module": [[13, "module-src.model_classes.HashingVectorizerFE"]], "ModelManager module": [[14, "module-src.model_classes.ModelManager"]], "ModelWorker module": [[15, "module-src.model_classes.ModelWorker"]], "StaticClassifier module": [[16, "module-src.model_classes.StaticClassifier"]], "TfidfVectorizerFE module": [[17, "module-src.model_classes.TfidfVectorizerFE"]], "model_classes": [[18, "model-classes"]], "Welcome to MLO-DCM\u2019s documentation!": [[19, "welcome-to-mlo-dcm-s-documentation"]], "Contents:": [[19, null]], "Indices and tables": [[19, "indices-and-tables"]]}, "indexentries": {"module": [[0, "module-src.main.consts_values"], [1, "module-src.main.io_utilities"], [2, "module-src.main.main"], [3, "module-src.main.model_utilities"], [5, "module-src.main.preprocessing_flow"], [6, "module-src.main.save_model_bento_ml"], [7, "module-src.main.text_preprocessing_utilities"], [8, "module-src.main.unit_test_model_utilities"], [9, "module-src.main.unit_test_text_preprocessing_utilities"], [10, "module-src.model_classes.CountVectorizerFE"], [11, "module-src.model_classes.Doc2VecFE"], [12, "module-src.model_classes.FeaturesExtractor"], [13, "module-src.model_classes.HashingVectorizerFE"], [14, "module-src.model_classes.ModelManager"], [15, "module-src.model_classes.ModelWorker"], [16, "module-src.model_classes.StaticClassifier"], [17, "module-src.model_classes.TfidfVectorizerFE"]], "src.main.consts_values": [[0, "module-src.main.consts_values"]], "export_as_binary_obj() (in module src.main.io_utilities)": [[1, "src.main.io_utilities.export_as_binary_obj"]], "import_binary_object() (in module src.main.io_utilities)": [[1, "src.main.io_utilities.import_binary_object"]], "read_raw_data() (in module src.main.io_utilities)": [[1, "src.main.io_utilities.read_raw_data"]], "read_txt_file() (in module src.main.io_utilities)": [[1, "src.main.io_utilities.read_txt_file"]], "save_dict_to_json_file() (in module src.main.io_utilities)": [[1, "src.main.io_utilities.save_dict_to_json_file"]], "src.main.io_utilities": [[1, "module-src.main.io_utilities"]], "src.main.main": [[2, "module-src.main.main"]], "build_data_dictionary() (in module src.main.model_utilities)": [[3, "src.main.model_utilities.build_data_dictionary"]], "get_model_evaluation_metrics() (in module src.main.model_utilities)": [[3, "src.main.model_utilities.get_model_evaluation_metrics"]], "shuffle_dataframe() (in module src.main.model_utilities)": [[3, "src.main.model_utilities.shuffle_dataframe"]], "split_model_data() (in module src.main.model_utilities)": [[3, "src.main.model_utilities.split_model_data"]], "src.main.model_utilities": [[3, "module-src.main.model_utilities"]], "vocabulary_dict_to_json() (in module src.main.model_utilities)": [[3, "src.main.model_utilities.vocabulary_dict_to_json"]], "apply_custom_tokenizer_iteratively() (in module src.main.preprocessing_flow)": [[5, "src.main.preprocessing_flow.apply_custom_tokenizer_iteratively"]], "custom_tokenizer() (in module src.main.preprocessing_flow)": [[5, "src.main.preprocessing_flow.custom_tokenizer"]], "get_nlp_model() (in module src.main.preprocessing_flow)": [[5, "src.main.preprocessing_flow.get_nlp_model"]], "preprocess_file() (in module src.main.preprocessing_flow)": [[5, "src.main.preprocessing_flow.preprocess_file"]], "process_df() (in module src.main.preprocessing_flow)": [[5, "src.main.preprocessing_flow.process_df"]], "read_preprocess_and_export() (in module src.main.preprocessing_flow)": [[5, "src.main.preprocessing_flow.read_preprocess_and_export"]], "src.main.preprocessing_flow": [[5, "module-src.main.preprocessing_flow"]], "src.main.save_model_bento_ml": [[6, "module-src.main.save_model_bento_ml"]], "get_lowercase_words_from_str() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.get_lowercase_words_from_str"]], "get_rare_tokens() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.get_rare_tokens"]], "get_spacy_tokens_from_raw_text() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.get_spacy_tokens_from_raw_text"]], "get_str_stopwords() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.get_str_stopwords"]], "get_str_tokens_freq() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.get_str_tokens_freq"]], "get_str_tokens_freq_for_lists() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.get_str_tokens_freq_for_lists"]], "handle_rare_str_tokens() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.handle_rare_str_tokens"]], "is_6digits_date() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.is_6digits_date"]], "is_str_fraction() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.is_str_fraction"]], "is_str_numeric() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.is_str_numeric"]], "is_str_valid_date() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.is_str_valid_date"]], "is_valid_resource() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.is_valid_resource"]], "is_valid_url() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.is_valid_url"]], "lemmatize_spacy_tokens() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.lemmatize_spacy_tokens"]], "remove_excessive_space() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.remove_excessive_space"]], "remove_spacy_punctuations() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.remove_spacy_punctuations"]], "remove_str_tokens_len_less_than_threshold() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.remove_str_tokens_len_less_than_threshold"]], "spacy_tokens_pos() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.spacy_tokens_pos"]], "spacy_tokens_to_str_tokens() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.spacy_tokens_to_str_tokens"]], "split_and_gather_str_tokens_by_separator() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.split_and_gather_str_tokens_by_separator"]], "src.main.text_preprocessing_utilities": [[7, "module-src.main.text_preprocessing_utilities"]], "str_6digits_dates_to_date_tag() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_6digits_dates_to_date_tag"]], "str_currency_to_spoken_words() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_currency_to_spoken_words"]], "str_dates_to_date_tag() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_dates_to_date_tag"]], "str_emails_to_email_tag() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_emails_to_email_tag"]], "str_fraction_to_spoken_words() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_fraction_to_spoken_words"]], "str_initial_case_to_tag() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_initial_case_to_tag"]], "str_number_with_separators_to_integer_number() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_number_with_separators_to_integer_number"]], "str_numeric_values_to_spoken_words() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_numeric_values_to_spoken_words"]], "str_ordinal_numbers_to_spoken_words() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_ordinal_numbers_to_spoken_words"]], "str_remove_common_chars() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_remove_common_chars"]], "str_remove_junk_spaces() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_remove_junk_spaces"]], "str_tokens_numbers_with_separators_to_spoken_words() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_tokens_numbers_with_separators_to_spoken_words"]], "str_tokens_remove_stopwords() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_tokens_remove_stopwords"]], "str_tokens_replace_symbol_with_tag() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_tokens_replace_symbol_with_tag"]], "str_tokens_to_spacy_tokens() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_tokens_to_spacy_tokens"]], "str_tokens_to_str() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_tokens_to_str"]], "str_urls_to_url_tag() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_urls_to_url_tag"]], "str_years_to_spoken_words() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.str_years_to_spoken_words"]], "to_lowercase() (in module src.main.text_preprocessing_utilities)": [[7, "src.main.text_preprocessing_utilities.to_lowercase"]], "unittests (class in src.main.unit_test_model_utilities)": [[8, "src.main.unit_test_model_utilities.UnitTests"]], "src.main.unit_test_model_utilities": [[8, "module-src.main.unit_test_model_utilities"]], "test_build_data_dictionary() (src.main.unit_test_model_utilities.unittests method)": [[8, "src.main.unit_test_model_utilities.UnitTests.test_build_data_dictionary"]], "test_get_model_evaluation_metrics() (src.main.unit_test_model_utilities.unittests method)": [[8, "src.main.unit_test_model_utilities.UnitTests.test_get_model_evaluation_metrics"]], "test_shuffle_dataframe() (src.main.unit_test_model_utilities.unittests method)": [[8, "src.main.unit_test_model_utilities.UnitTests.test_shuffle_dataframe"]], "test_split_model_data() (src.main.unit_test_model_utilities.unittests method)": [[8, "src.main.unit_test_model_utilities.UnitTests.test_split_model_data"]], "test_vocabulary_dict_to_json() (src.main.unit_test_model_utilities.unittests method)": [[8, "src.main.unit_test_model_utilities.UnitTests.test_vocabulary_dict_to_json"]], "unittests (class in src.main.unit_test_text_preprocessing_utilities)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests"]], "src.main.unit_test_text_preprocessing_utilities": [[9, "module-src.main.unit_test_text_preprocessing_utilities"]], "test_get_lowercase_words_from_str() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_get_lowercase_words_from_str"]], "test_get_rare_tokens() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_get_rare_tokens"]], "test_get_spacy_tokens_from_raw_text() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_get_spacy_tokens_from_raw_text"]], "test_get_str_tokens_freq() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_get_str_tokens_freq"]], "test_get_str_tokens_freq_for_lists() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_get_str_tokens_freq_for_lists"]], "test_handle_rare_str_tokens() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_handle_rare_str_tokens"]], "test_is_6digits_date() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_is_6digits_date"]], "test_is_str_fraction() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_is_str_fraction"]], "test_is_str_numeric() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_is_str_numeric"]], "test_is_str_valid_date() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_is_str_valid_date"]], "test_is_valid_resource() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_is_valid_resource"]], "test_is_valid_url() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_is_valid_url"]], "test_lemmatize_spacy_tokens() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_lemmatize_spacy_tokens"]], "test_remove_excessive_space() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_remove_excessive_space"]], "test_remove_spacy_punctuations() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_remove_spacy_punctuations"]], "test_remove_str_tokens_len_less_than_threshold() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_remove_str_tokens_len_less_than_threshold"]], "test_spacy_tokens_pos() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_spacy_tokens_pos"]], "test_spacy_tokens_to_str_tokens() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_spacy_tokens_to_str_tokens"]], "test_split_and_gather_str_tokens_by_separator() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_split_and_gather_str_tokens_by_separator"]], "test_str_6digits_dates_to_date_tag() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_6digits_dates_to_date_tag"]], "test_str_currency_to_spoken_words() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_currency_to_spoken_words"]], "test_str_dates_to_date_tag() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_dates_to_date_tag"]], "test_str_emails_to_email_tag() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_emails_to_email_tag"]], "test_str_fraction_to_spoken_words() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_fraction_to_spoken_words"]], "test_str_initial_case_to_tag() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_initial_case_to_tag"]], "test_str_number_with_separators_to_integer_number() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_number_with_separators_to_integer_number"]], "test_str_numeric_values_to_spoken_words() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_numeric_values_to_spoken_words"]], "test_str_ordinal_numbers_to_spoken_words() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_ordinal_numbers_to_spoken_words"]], "test_str_remove_common_chars() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_remove_common_chars"]], "test_str_remove_junk_spaces() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_remove_junk_spaces"]], "test_str_tokens_numbers_with_separators_to_spoken_words() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_tokens_numbers_with_separators_to_spoken_words"]], "test_str_tokens_remove_stopwords() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_tokens_remove_stopwords"]], "test_str_tokens_replace_quote_with_tag() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_tokens_replace_quote_with_tag"]], "test_str_tokens_to_spacy_tokens() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_tokens_to_spacy_tokens"]], "test_str_tokens_to_str() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_tokens_to_str"]], "test_str_urls_to_url_tag() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_urls_to_url_tag"]], "test_str_years_to_spoken_words() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_str_years_to_spoken_words"]], "test_to_lowercase() (src.main.unit_test_text_preprocessing_utilities.unittests method)": [[9, "src.main.unit_test_text_preprocessing_utilities.UnitTests.test_to_lowercase"]], "countvectorizerfe (class in src.model_classes.countvectorizerfe)": [[10, "src.model_classes.CountVectorizerFE.CountVectorizerFE"]], "get_extractor_params() (src.model_classes.countvectorizerfe.countvectorizerfe method)": [[10, "src.model_classes.CountVectorizerFE.CountVectorizerFE.get_extractor_params"]], "get_vocabulary() (src.model_classes.countvectorizerfe.countvectorizerfe method)": [[10, "src.model_classes.CountVectorizerFE.CountVectorizerFE.get_vocabulary"]], "set_extractor_params() (src.model_classes.countvectorizerfe.countvectorizerfe method)": [[10, "src.model_classes.CountVectorizerFE.CountVectorizerFE.set_extractor_params"]], "short_str() (src.model_classes.countvectorizerfe.countvectorizerfe method)": [[10, "src.model_classes.CountVectorizerFE.CountVectorizerFE.short_str"]], "src.model_classes.countvectorizerfe": [[10, "module-src.model_classes.CountVectorizerFE"]], "transform_data() (src.model_classes.countvectorizerfe.countvectorizerfe method)": [[10, "src.model_classes.CountVectorizerFE.CountVectorizerFE.transform_data"]], "doc2vecfe (class in src.model_classes.doc2vecfe)": [[11, "src.model_classes.Doc2VecFE.Doc2VecFE"]], "get_extractor_params() (src.model_classes.doc2vecfe.doc2vecfe method)": [[11, "src.model_classes.Doc2VecFE.Doc2VecFE.get_extractor_params"]], "set_extractor_params() (src.model_classes.doc2vecfe.doc2vecfe method)": [[11, "src.model_classes.Doc2VecFE.Doc2VecFE.set_extractor_params"]], "short_str() (src.model_classes.doc2vecfe.doc2vecfe method)": [[11, "src.model_classes.Doc2VecFE.Doc2VecFE.short_str"]], "src.model_classes.doc2vecfe": [[11, "module-src.model_classes.Doc2VecFE"]], "transform_data() (src.model_classes.doc2vecfe.doc2vecfe method)": [[11, "src.model_classes.Doc2VecFE.Doc2VecFE.transform_data"]], "featuresextractor (class in src.model_classes.featuresextractor)": [[12, "src.model_classes.FeaturesExtractor.FeaturesExtractor"]], "get_data() (src.model_classes.featuresextractor.featuresextractor method)": [[12, "src.model_classes.FeaturesExtractor.FeaturesExtractor.get_data"]], "get_extractor_params() (src.model_classes.featuresextractor.featuresextractor method)": [[12, "src.model_classes.FeaturesExtractor.FeaturesExtractor.get_extractor_params"]], "get_vocabulary() (src.model_classes.featuresextractor.featuresextractor method)": [[12, "src.model_classes.FeaturesExtractor.FeaturesExtractor.get_vocabulary"]], "set_data() (src.model_classes.featuresextractor.featuresextractor method)": [[12, "src.model_classes.FeaturesExtractor.FeaturesExtractor.set_data"]], "set_extractor_params() (src.model_classes.featuresextractor.featuresextractor method)": [[12, "src.model_classes.FeaturesExtractor.FeaturesExtractor.set_extractor_params"]], "src.model_classes.featuresextractor": [[12, "module-src.model_classes.FeaturesExtractor"]], "transform_data() (src.model_classes.featuresextractor.featuresextractor method)": [[12, "src.model_classes.FeaturesExtractor.FeaturesExtractor.transform_data"]], "hashingvectorizerfe (class in src.model_classes.hashingvectorizerfe)": [[13, "src.model_classes.HashingVectorizerFE.HashingVectorizerFE"]], "get_extractor_params() (src.model_classes.hashingvectorizerfe.hashingvectorizerfe method)": [[13, "src.model_classes.HashingVectorizerFE.HashingVectorizerFE.get_extractor_params"]], "get_vocabulary() (src.model_classes.hashingvectorizerfe.hashingvectorizerfe method)": [[13, "src.model_classes.HashingVectorizerFE.HashingVectorizerFE.get_vocabulary"]], "set_extractor_params() (src.model_classes.hashingvectorizerfe.hashingvectorizerfe method)": [[13, "src.model_classes.HashingVectorizerFE.HashingVectorizerFE.set_extractor_params"]], "short_str() (src.model_classes.hashingvectorizerfe.hashingvectorizerfe method)": [[13, "src.model_classes.HashingVectorizerFE.HashingVectorizerFE.short_str"]], "src.model_classes.hashingvectorizerfe": [[13, "module-src.model_classes.HashingVectorizerFE"]], "transform_data() (src.model_classes.hashingvectorizerfe.hashingvectorizerfe method)": [[13, "src.model_classes.HashingVectorizerFE.HashingVectorizerFE.transform_data"]], "build_classifiers() (in module src.model_classes.modelmanager)": [[14, "src.model_classes.ModelManager.build_classifiers"]], "build_features_extractors() (in module src.model_classes.modelmanager)": [[14, "src.model_classes.ModelManager.build_features_extractors"]], "get_classifier_to_extractor_str() (in module src.model_classes.modelmanager)": [[14, "src.model_classes.ModelManager.get_classifier_to_extractor_str"]], "manager_execute() (in module src.model_classes.modelmanager)": [[14, "src.model_classes.ModelManager.manager_execute"]], "model_fit_train_predict() (in module src.model_classes.modelmanager)": [[14, "src.model_classes.ModelManager.model_fit_train_predict"]], "reverse_classifier_to_extractor_str() (in module src.model_classes.modelmanager)": [[14, "src.model_classes.ModelManager.reverse_classifier_to_extractor_str"]], "save_model_component() (in module src.model_classes.modelmanager)": [[14, "src.model_classes.ModelManager.save_model_component"]], "src.model_classes.modelmanager": [[14, "module-src.model_classes.ModelManager"]], "evaluate_classifiers() (in module src.model_classes.modelworker)": [[15, "src.model_classes.ModelWorker.evaluate_classifiers"]], "import_classifiers() (in module src.model_classes.modelworker)": [[15, "src.model_classes.ModelWorker.import_classifiers"]], "import_features_extractors() (in module src.model_classes.modelworker)": [[15, "src.model_classes.ModelWorker.import_features_extractors"]], "import_model_objects() (in module src.model_classes.modelworker)": [[15, "src.model_classes.ModelWorker.import_model_objects"]], "perform_prediction() (in module src.model_classes.modelworker)": [[15, "src.model_classes.ModelWorker.perform_prediction"]], "predict_input_from_file() (in module src.model_classes.modelworker)": [[15, "src.model_classes.ModelWorker.predict_input_from_file"]], "preprocess_input() (in module src.model_classes.modelworker)": [[15, "src.model_classes.ModelWorker.preprocess_input"]], "src.model_classes.modelworker": [[15, "module-src.model_classes.ModelWorker"]], "voting_system() (in module src.model_classes.modelworker)": [[15, "src.model_classes.ModelWorker.voting_system"]], "worker_execute() (in module src.model_classes.modelworker)": [[15, "src.model_classes.ModelWorker.worker_execute"]], "staticclassifier (class in src.model_classes.staticclassifier)": [[16, "src.model_classes.StaticClassifier.StaticClassifier"]], "fit_train_evaluate() (src.model_classes.staticclassifier.staticclassifier method)": [[16, "src.model_classes.StaticClassifier.StaticClassifier.fit_train_evaluate"]], "get_confusion_matrix() (src.model_classes.staticclassifier.staticclassifier method)": [[16, "src.model_classes.StaticClassifier.StaticClassifier.get_confusion_matrix"]], "get_model_classes() (src.model_classes.staticclassifier.staticclassifier method)": [[16, "src.model_classes.StaticClassifier.StaticClassifier.get_model_classes"]], "get_model_params() (src.model_classes.staticclassifier.staticclassifier method)": [[16, "src.model_classes.StaticClassifier.StaticClassifier.get_model_params"]], "predict() (src.model_classes.staticclassifier.staticclassifier method)": [[16, "src.model_classes.StaticClassifier.StaticClassifier.predict"]], "predict_probabilities() (src.model_classes.staticclassifier.staticclassifier method)": [[16, "src.model_classes.StaticClassifier.StaticClassifier.predict_probabilities"]], "set_model_params() (src.model_classes.staticclassifier.staticclassifier method)": [[16, "src.model_classes.StaticClassifier.StaticClassifier.set_model_params"]], "short_str() (src.model_classes.staticclassifier.staticclassifier method)": [[16, "src.model_classes.StaticClassifier.StaticClassifier.short_str"]], "src.model_classes.staticclassifier": [[16, "module-src.model_classes.StaticClassifier"]], "tfidfvectorizerfe (class in src.model_classes.tfidfvectorizerfe)": [[17, "src.model_classes.TfidfVectorizerFE.TfidfVectorizerFE"]], "get_extractor_params() (src.model_classes.tfidfvectorizerfe.tfidfvectorizerfe method)": [[17, "src.model_classes.TfidfVectorizerFE.TfidfVectorizerFE.get_extractor_params"]], "get_vocabulary() (src.model_classes.tfidfvectorizerfe.tfidfvectorizerfe method)": [[17, "src.model_classes.TfidfVectorizerFE.TfidfVectorizerFE.get_vocabulary"]], "set_extractor_params() (src.model_classes.tfidfvectorizerfe.tfidfvectorizerfe method)": [[17, "src.model_classes.TfidfVectorizerFE.TfidfVectorizerFE.set_extractor_params"]], "short_str() (src.model_classes.tfidfvectorizerfe.tfidfvectorizerfe method)": [[17, "src.model_classes.TfidfVectorizerFE.TfidfVectorizerFE.short_str"]], "src.model_classes.tfidfvectorizerfe": [[17, "module-src.model_classes.TfidfVectorizerFE"]], "transform_data() (src.model_classes.tfidfvectorizerfe.tfidfvectorizerfe method)": [[17, "src.model_classes.TfidfVectorizerFE.TfidfVectorizerFE.transform_data"]]}})