{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf42d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6591d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm   NOT HERE, FROM CONDA CONSOLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad4fd5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb8b5378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric_str(s):\n",
    "    try:\n",
    "        # try converting to float\n",
    "        float_value = float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # try converting to int\n",
    "            int_value = int(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68f3ab1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text and this is another one\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing functions\n",
    "\n",
    "# The strategy is to use functions without side effects - so do not modify the passes object itself, construct a new way\n",
    "# that will be returned\n",
    "\n",
    "nlp_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_excessive_space(text):\n",
    "    '''\n",
    "    Remove excessive white spaces like \" \", \\n, \\t from the beginning and ending of text\n",
    "    \n",
    "    :param text - input text; it's a native python string\n",
    "    :return: the given text without spaces; \n",
    "    :rtype: built-in python string\n",
    "   \n",
    "    '''\n",
    "    return text.strip()\n",
    "\n",
    "print(remove_excessive_space(\"\\n\\n This is a text and this is another one \\n \\n \\t\"))\n",
    "\n",
    "\n",
    "def remove_punctuations(words):\n",
    "    '''\n",
    "    Remove all the punctuations from the given text\n",
    "    \n",
    "    :param words: the input list that contains all the words and punctuations\n",
    "    :return: a list with all words, without punctuations; \n",
    "    :rtype: built-in python list\n",
    "    '''\n",
    "    \n",
    "    words_res = []\n",
    "    for word in words:\n",
    "        if word.is_punct is False:\n",
    "            words_res.append(word)\n",
    "    \n",
    "    return words_res\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    '''\n",
    "    Remove all the stop words from the given text\n",
    "    \n",
    "    :param words: the input list that contains all the words\n",
    "    :return: the given list, without stop words; \n",
    "    :rtype: built-in python list\n",
    "    '''\n",
    "    words_res = []\n",
    "    for word in words:\n",
    "        if word.is_stop is False:\n",
    "            words_res.append(word)\n",
    "    \n",
    "    return words_res\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    '''\n",
    "    Apply lemmatization for a list of words. \n",
    "    \n",
    "    :param words: the input list with words; every element is a spacy.tokens.token.Token object\n",
    "    :return: a list constructed from the initial one but every with is lemmatized (converted to base form)\n",
    "    :rtype: built-in python list, every element is a spacy.tokens.token.Token object\n",
    "    '''\n",
    "    \n",
    "    words_res = []\n",
    "    for word in words:\n",
    "        words_res.append(word.lemma_)\n",
    "    \n",
    "    return words_res\n",
    "\n",
    "def handle_numerical_values(word, method='text'):\n",
    "    '''\n",
    "   Decide what to do with numerical values (keep them or remove them)\n",
    "   \n",
    "   :param word:\n",
    "   :param method:\n",
    "   '''\n",
    "    if method == 'text':\n",
    "        word = re.sub(r'\\d+', 'NUM', word)\n",
    "    elif method == 'remove':\n",
    "        word = re.sub(r'\\d+', '', word)\n",
    "    return word\n",
    "   \n",
    "def handle_rare_words_and_typos(words, threshold=2, replacement='[UNK]'):\n",
    "    '''\n",
    "   Decide if we want to replace rare words or not\n",
    "   \n",
    "   :param text:\n",
    "   :param threshold:\n",
    "   :param replacement:\n",
    "   :return:\n",
    "   :rtype:\n",
    "   '''\n",
    "    word_freq = {word: words.count(word) for word in set(words)}\n",
    "    rare_words = [word for word, freq in word_freq.items() if freq <= threshold]\n",
    "    \n",
    "    processed_words = [replacement if word in rare_words else word for word in words]\n",
    "    \n",
    "    return processed_words\n",
    "    \n",
    "\n",
    "# part of speech for every word\n",
    "def words_pos(words):\n",
    "    words_res = []\n",
    "    for word in words:\n",
    "        words_res.append( (word, word.pos_) ) \n",
    "    \n",
    "    return words_res\n",
    "\n",
    "def get_tokens_from_raw_text(text, nlp_model):\n",
    "    '''\n",
    "    Convert a raw text to a built-in python list of spacy.tokens.token.Token object (tokens); \n",
    "    \n",
    "    :param text: the input text; it's a native python string\n",
    "    :param nlp_model: NLP model that is used to preprocess the text; it's a spacy.lang object\n",
    "    :return: list of words extracted from the input text\n",
    "    :rtype: built-in python list\n",
    "    '''\n",
    "    doc = nlp_model(text)\n",
    "    words = []\n",
    "    for token in doc:\n",
    "        words.append(token)\n",
    "        \n",
    "    return words\n",
    "\n",
    "def remove_junk_spaces(tokens, nlp_model):\n",
    "    \n",
    "    # convert spacy tokens to str tokens\n",
    "    tokens = convert_spacy_tokens_to_str_tokens(tokens)\n",
    "    \n",
    "    # remove extra spaces with strip\n",
    "    tokens = [remove_excessive_space(token) for token in tokens]\n",
    "    \n",
    "    \n",
    "    junk_spaces = ['\\n', '\\t', '\\r', '\\v', '\\f', '&nbsp;', '\\xA0', '', ' ']\n",
    "    \n",
    "    # remove other junk spaces\n",
    "    tokens = [token for token in tokens if token not in junk_spaces]\n",
    "    \n",
    "    tokens = convert_str_tokens_to_spacy_tokens(tokens, nlp_model)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "def convert_years_to_spoken_words(tokens, nlp_model):\n",
    "    # convert years to spoken words, eg. \"1990\" to 'nineteen ninety'\n",
    "    # we consider years as integer values with 4 digits, and the value itself\n",
    "    # between valid_year_min_value to valid_year_max_value\n",
    "    \n",
    "    valid_year_min_value = 1000\n",
    "    valid_year_max_value = 2100\n",
    "    \n",
    "    # convert tokens from spacy entity to built-in string\n",
    "    tokens = convert_spacy_tokens_to_str_tokens(tokens)\n",
    "    new_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token.isnumeric() and len(token) == 4:\n",
    "            year = int(token)\n",
    "            if year >= valid_year_min_value and year <= valid_year_max_value:\n",
    "                year_as_words = num2words(year, to = 'year')\n",
    "                new_tokens.append(year_as_words)\n",
    "            # logica\n",
    "        else:\n",
    "            # just append it like this\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    new_tokens = convert_str_tokens_to_spacy_tokens(new_tokens, nlp_model)\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "def convert_numeric_values_to_spoken_words(tokens, nlp_model):\n",
    "    # conver numerical values (eg. '54', '2.5') to spoken words\n",
    "    \n",
    "    new_tokens = []\n",
    "    \n",
    "    # convert tokens from spacy entity to built-in string\n",
    "    tokens = convert_spacy_tokens_to_str_tokens(tokens)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if is_numeric_str(token):\n",
    "            \n",
    "            token_as_numeric = float(token)\n",
    "            token_as_spoken_words = num2words(token_as_numeric)\n",
    "            new_tokens.append(token_as_spoken_words)\n",
    "            if token == '0.30':\n",
    "                print(token_as_numeric)\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    new_tokens = convert_str_tokens_to_spacy_tokens(new_tokens, nlp_model)\n",
    "    \n",
    "    return new_tokens\n",
    "    \n",
    "\n",
    "def convert_str_tokens_to_spacy_tokens(tokens, nlp_model):\n",
    "    # convert string tokens back into spacy entities\n",
    "    raw_text = ' '.join(tokens)\n",
    "    tokens = get_tokens_from_raw_text(raw_text, nlp_model)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def convert_spacy_tokens_to_str_tokens(tokens):\n",
    "    # convert tokens from spacy entity to built-in string\n",
    "    tokens = [token.text for token in tokens]\n",
    "    return tokens\n",
    "    \n",
    "    \n",
    "# input_text = \"The quick brown foxes are jumping over the lazy dogs. They were running through the forests, exploring the mysterious caves. I saw many interesting books on the shelves and decided to read them all. The children were playing happily in the parks, swinging on the swings and climbing on the jungle gym. Despite the challenges, they were determined to succeed in their endeavors.\"\n",
    "# words_input = get_tokens_from_raw_text(input_text, nlp_model)\n",
    "# print(\"Tokens:\")\n",
    "# print(words_input)\n",
    "# print(\"-\" * 15)\n",
    "# words_input = remove_punctuations(words_input)\n",
    "# print(\"Without punctuations:\")\n",
    "# print(words_input)\n",
    "# print(\"-\" * 15)\n",
    "# words_input = remove_stopwords(words_input)\n",
    "# print(\"Without stopwords:\")\n",
    "# print(words_input)\n",
    "# print(\"-\" * 15)\n",
    "# words_input = lemmatize_words(words_input)\n",
    "# print(\"After lemmatization:\")\n",
    "# print(words_input)\n",
    "# print(\"-\" * 25)\n",
    "\n",
    "# # need to convert again to text and then to tokenize because the lemmatization convert words to built in string\n",
    "# words_as_single_text = ' '.join(words_input)\n",
    "# words_input = get_tokens_from_raw_text(words_as_single_text, nlp_model)\n",
    "# words_and_pos = words_pos(words_input)\n",
    "# print(\"Part of speech:\")\n",
    "# print(words_and_pos)\n",
    "# print(\"-\" * 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73948622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IO functions\n",
    "def read_txt_file(file_path):\n",
    "    '''\n",
    "    Return the content from the file from the given path. We assume the first line is the document title and the\n",
    "    second line is document content\n",
    "    \n",
    "    :param file_path: path to the target file \n",
    "    :return: a dictionary with 2 entries: title and content of the file\n",
    "    :rtype: built-in python dictionary\n",
    "    '''\n",
    "    result = dict()\n",
    "    with open(file_path, 'r', encoding='utf-8') as file_obj:  \n",
    "                result['title'] = file_obj.readline()\n",
    "                result['content'] = file_obj.read()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c04d762d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lufthansa flies back to profit\\n</td>\n",
       "      <td>\\nGerman airline Lufthansa has returned to pro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Winn-Dixie files for bankruptcy\\n</td>\n",
       "      <td>\\nUS supermarket group Winn-Dixie has filed fo...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US economy still growing says Fed\\n</td>\n",
       "      <td>\\nMost areas of the US saw their economy conti...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saab to build Cadillacs in Sweden\\n</td>\n",
       "      <td>\\nGeneral Motors, the world's largest car make...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bank voted 8-1 for no rate change\\n</td>\n",
       "      <td>\\nThe decision to keep interest rates on hold ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Mobile games come of age\\n</td>\n",
       "      <td>\\nThe BBC News website takes a look at how gam...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>California sets fines for spyware\\n</td>\n",
       "      <td>\\nThe makers of computer programs that secretl...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Web helps collect aid donations\\n</td>\n",
       "      <td>\\nThe web is helping aid agencies gather resou...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Mobiles rack up 20 years of use\\n</td>\n",
       "      <td>\\nMobile phones in the UK are celebrating thei...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Blogs take on the mainstream\\n</td>\n",
       "      <td>\\nWeb logs or blogs are everywhere, with at le...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title  \\\n",
       "0       Lufthansa flies back to profit\\n   \n",
       "1      Winn-Dixie files for bankruptcy\\n   \n",
       "2    US economy still growing says Fed\\n   \n",
       "3    Saab to build Cadillacs in Sweden\\n   \n",
       "4    Bank voted 8-1 for no rate change\\n   \n",
       "..                                   ...   \n",
       "995           Mobile games come of age\\n   \n",
       "996  California sets fines for spyware\\n   \n",
       "997    Web helps collect aid donations\\n   \n",
       "998    Mobiles rack up 20 years of use\\n   \n",
       "999       Blogs take on the mainstream\\n   \n",
       "\n",
       "                                               content        type  \n",
       "0    \\nGerman airline Lufthansa has returned to pro...    business  \n",
       "1    \\nUS supermarket group Winn-Dixie has filed fo...    business  \n",
       "2    \\nMost areas of the US saw their economy conti...    business  \n",
       "3    \\nGeneral Motors, the world's largest car make...    business  \n",
       "4    \\nThe decision to keep interest rates on hold ...    business  \n",
       "..                                                 ...         ...  \n",
       "995  \\nThe BBC News website takes a look at how gam...  technology  \n",
       "996  \\nThe makers of computer programs that secretl...  technology  \n",
       "997  \\nThe web is helping aid agencies gather resou...  technology  \n",
       "998  \\nMobile phones in the UK are celebrating thei...  technology  \n",
       "999  \\nWeb logs or blogs are everywhere, with at le...  technology  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_raw_data(main_directory_path):\n",
    "    \" read all files from all directories from the given path;  return a pandas df with 3 columns: document title, content and type (label) \"\n",
    "    df = pd.DataFrame(columns=['title','content','type'])\n",
    "    directories = os.listdir(main_directory_path)\n",
    "    \n",
    "    new_files_contents = []\n",
    "    \n",
    "    for directory in directories:\n",
    "        directory_path = main_directory_path + \"\\\\\" + directory\n",
    "        files = os.listdir(directory_path)\n",
    "        for file in files:\n",
    "            file_path = directory_path + \"\\\\\" + file\n",
    "            file_content = read_txt_file(file_path)\n",
    "    \n",
    "            whole_file_content_as_dict = pd.DataFrame({'title':file_content['title'], 'content':file_content['content'], 'type':directory}, index = [0])\n",
    "            new_files_contents.append(whole_file_content_as_dict)\n",
    "                   \n",
    "    df = pd.concat([df] + new_files_contents, ignore_index=True)\n",
    "            \n",
    "    return df\n",
    "\n",
    "data_root_path = \"data\"\n",
    "df = read_raw_data(data_root_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da8c4606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "German airline Lufthansa has returned to profit in 2004 after posting huge losses in 2003.\n",
      "\n",
      "In a preliminary report, the airline announced net profits of 400m euros ($527.61m; £274.73m), compared with a loss of 984m euros in 2003. Operating profits were at 380m euros, ten times more than in 2003. Lufthansa was hit in 2003 by tough competition and a dip in demand following the Iraq war and the killer SARS virus. It was also hit by troubles at its US catering business. Last year, Lufthansa showed signs of recovery even as some European and US airlines were teetering on the brink of bankruptcy. The board of Lufthansa has recommended paying a 2004 dividend of 0.30 euros per share. In 2003, shareholders did not get a dividend. The company said that it will give all the details of its 2004 results on 23 March.\n",
      "\n",
      "0.3\n",
      "<class 'list'>\n",
      "['german', 'airline', 'Lufthansa', 'return', 'profit', 'thousand', 'post', 'huge', 'loss', 'thousand', 'preliminary', 'report', 'airline', 'announce', 'net', 'profit', 'm', 'euro', '$', 'seven', 'point', 'm', '£', 'seventy', 'point', 'seven', 'm', 'compare', 'loss', 'eighty', 'm', 'euro', 'thousand', 'operate', 'profit', 'eighty', 'm', 'euro', 'time', 'thousand', 'Lufthansa', 'hit', 'thousand', 'tough', 'competition', 'dip', 'demand', 'follow', 'Iraq', 'war', 'killer', 'SARS', 'virus', 'hit', 'trouble', 'catering', 'business', 'year', 'Lufthansa', 'show', 'sign', 'recovery', 'european', 'airline', 'teeter', 'brink', 'bankruptcy', 'board', 'Lufthansa', 'recommend', 'pay', 'thousand', 'dividend', 'zero', 'point', 'euro', 'share', 'thousand', 'shareholder', 'dividend', 'company', 'say', 'detail', 'thousand', 'result', 'March']\n"
     ]
    }
   ],
   "source": [
    "# dummy text classification\n",
    "\n",
    "# load\n",
    "nlp_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "first_doc = df.iloc[0]['content']\n",
    "\n",
    "# first_doc = remove_excessive_space(first_doc)\n",
    "\n",
    "# tokens = get_tokens_from_raw_text(first_doc, nlp_model)\n",
    "# print(first_doc)\n",
    "# print(tokens)\n",
    "\n",
    "print(first_doc)\n",
    "\n",
    "def custom_tokenizer(raw_text, nlp_model):\n",
    "    \n",
    "    # convert to lower case\n",
    "    #raw_text = to_lowercase(raw_text)\n",
    "    \n",
    "    # remove extra spaces in the first phase\n",
    "    raw_text = remove_excessive_space(raw_text)\n",
    "    \n",
    "    # get tokens\n",
    "    tokens = get_tokens_from_raw_text(raw_text, nlp_model)\n",
    "    \n",
    "    # remove junk extra spaces\n",
    "    tokens = remove_junk_spaces(tokens, nlp_model)\n",
    "      \n",
    "    # handle years value - convert years as numerical value into spoken words\n",
    "    tokens = convert_years_to_spoken_words(tokens, nlp_model)\n",
    "    \n",
    "    # convert currency symbols into spoken words - MAYBE NOT, just removed them\n",
    "    \n",
    "    # convert articulated date into spoken words (e.g '3rd' -> 'third')\n",
    "    \n",
    "    # convert the left numerical values (int, float) into spoken words\n",
    "    tokens = convert_numeric_values_to_spoken_words(tokens, nlp_model)\n",
    "    \n",
    "    # remove punctuations\n",
    "    tokens = remove_punctuations(tokens)\n",
    " \n",
    "    # remove stop words\n",
    "    tokens = remove_stopwords(tokens)\n",
    "\n",
    "    # lemmatization\n",
    "    tokens = lemmatize_words(tokens)\n",
    "    # after this, the tokens are not longer spacy.tokens.token.Token, but built-in java string\n",
    "    \n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "res_tokens = custom_tokenizer(first_doc, nlp_model)\n",
    "\n",
    "#res_tokens = np.array(res_tokens)\n",
    "print(type(res_tokens))\n",
    "print(res_tokens)\n",
    "\n",
    "\n",
    "# from num2words import num2words\n",
    "\n",
    "# print(num2words(1990, to = 'year'))\n",
    "# print(num2words(1990))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6424d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[German, airline, Lufthansa, has, returned, to, profit, in, 2004, after, posting, huge, losses, in, 2003, ., \n",
      "\n",
      " , In, a, preliminary, report, ,, the, airline, announced, net, profits, of, 400, m, euros, (, $, 527.61, m, ;, £, 274.73, m, ), ,, compared, with, a, loss, of, 984, m, euros, in, 2003, ., Operating, profits, were, at, 380, m, euros, ,, ten, times, more, than, in, 2003, ., Lufthansa, was, hit, in, 2003, by, tough, competition, and, a, dip, in, demand, following, the, Iraq, war, and, the, killer, SARS, virus, ., It, was, also, hit, by, troubles, at, its, US, catering, business, ., Last, year, ,, Lufthansa, showed, signs, of, recovery, even, as, some, European, and, US, airlines, were, teetering, on, the, brink, of, bankruptcy, ., The, board, of, Lufthansa, has, recommended, paying, a, 2004, dividend, of, 0.30, euros, per, share, ., In, 2003, ,, shareholders, did, not, get, a, dividend, ., The, company, said, that, it, will, give, all, the, details, of, its, 2004, results, on, 23, March, .]\n"
     ]
    }
   ],
   "source": [
    "text = df.iloc[0]['content']\n",
    "tokens = get_tokens_from_raw_text(text, nlp_model)\n",
    "tokens = remove_junk_spaces(tokens, nlp_model)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5dea1ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "two thousand and four\n",
      "twenty-third\n",
      "zero point three\n"
     ]
    }
   ],
   "source": [
    "# first_row = df.iloc[0]\n",
    "# content = first_row['content']\n",
    "# doc = nlp_model(content)\n",
    "# for sentence in doc.sents:\n",
    "#     print(sentence)\n",
    "\n",
    "print(type(num2words(1990, to = 'year')))\n",
    "print(num2words(2004 , to = 'year'))\n",
    "print(num2words(23, to = 'ordinal'))\n",
    "print(num2words(0.30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f78a7894",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__cinit__() takes exactly 3 positional arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToken\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\spacy\\tokens\\token.pyx:104\u001b[0m, in \u001b[0;36mspacy.tokens.token.Token.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __cinit__() takes exactly 3 positional arguments (1 given)"
     ]
    }
   ],
   "source": [
    "a = spacy.tokens.token.Token(\"da\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2694e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43f0dd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 101\n",
      "The 1109\n",
      "fox 17594\n",
      "is 1110\n",
      "running 1919\n",
      "run 1576\n",
      ". 119\n",
      "I 146\n",
      "saw 1486\n",
      "a 170\n",
      "running 1919\n",
      "fox 17594\n",
      ". 119\n",
      "[SEP] 102\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# Tokenize the text\n",
    "text = \"The fox is running run. I saw a running fox.\"\n",
    "#text = \"I left my phone on the left side of the room.\"\n",
    "encoded_text = tokenizer.encode(text)\n",
    "tokens = tokenizer.tokenize(tokenizer.decode(encoded_text))\n",
    "\n",
    "# Convert tokens to IDs\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Print the tokenized text and corresponding token IDs\n",
    "# print(\"Tokenized Text:\", tokens)\n",
    "# print(\"Token IDs:\", input_ids)\n",
    "\n",
    "for token, token_id in zip(tokens, input_ids):\n",
    "    print(token,token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d41dd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ec7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be21a9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
