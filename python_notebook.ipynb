{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc1e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4263173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm   NOT HERE, FROM CONDA CONSOLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8963576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d7258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text and this is another one\n",
      "Tokens:\n",
      "[The, quick, brown, foxes, are, jumping, over, the, lazy, dogs, ., They, were, running, through, the, forests, ,, exploring, the, mysterious, caves, ., I, saw, many, interesting, books, on, the, shelves, and, decided, to, read, them, all, ., The, children, were, playing, happily, in, the, parks, ,, swinging, on, the, swings, and, climbing, on, the, jungle, gym, ., Despite, the, challenges, ,, they, were, determined, to, succeed, in, their, endeavors, .]\n",
      "---------------\n",
      "Without punctuations:\n",
      "[The, quick, brown, foxes, are, jumping, over, the, lazy, dogs, They, were, running, through, the, forests, exploring, the, mysterious, caves, I, saw, many, interesting, books, on, the, shelves, and, decided, to, read, them, all, The, children, were, playing, happily, in, the, parks, swinging, on, the, swings, and, climbing, on, the, jungle, gym, Despite, the, challenges, they, were, determined, to, succeed, in, their, endeavors]\n",
      "---------------\n",
      "Without stopwords:\n",
      "[quick, brown, foxes, jumping, lazy, dogs, running, forests, exploring, mysterious, caves, saw, interesting, books, shelves, decided, read, children, playing, happily, parks, swinging, swings, climbing, jungle, gym, Despite, challenges, determined, succeed, endeavors]\n",
      "---------------\n",
      "After lemmatization:\n",
      "['quick', 'brown', 'fox', 'jump', 'lazy', 'dog', 'run', 'forest', 'explore', 'mysterious', 'cave', 'see', 'interesting', 'book', 'shelf', 'decide', 'read', 'child', 'play', 'happily', 'park', 'swinge', 'swing', 'climb', 'jungle', 'gym', 'despite', 'challenge', 'determined', 'succeed', 'endeavor']\n",
      "-------------------------\n",
      "Part of speech:\n",
      "[(quick, 'ADJ'), (brown, 'ADJ'), (fox, 'NOUN'), (jump, 'NOUN'), (lazy, 'ADJ'), (dog, 'NOUN'), (run, 'VERB'), (forest, 'NOUN'), (explore, 'VERB'), (mysterious, 'ADJ'), (cave, 'NOUN'), (see, 'VERB'), (interesting, 'ADJ'), (book, 'NOUN'), (shelf, 'NOUN'), (decide, 'VERB'), (read, 'VERB'), (child, 'NOUN'), (play, 'VERB'), (happily, 'ADV'), (park, 'NOUN'), (swinge, 'NOUN'), (swing, 'NOUN'), (climb, 'NOUN'), (jungle, 'NOUN'), (gym, 'NOUN'), (despite, 'SCONJ'), (challenge, 'NOUN'), (determined, 'ADJ'), (succeed, 'NOUN'), (endeavor, 'NOUN')]\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing functions\n",
    "\n",
    "# The strategy is to use functions without side effects - so do not modify the passes object itself, construct a new way\n",
    "# that will be returned\n",
    "\n",
    "nlp_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def remove_excessive_space(text):\n",
    "    '''\n",
    "    Remove excessive white spaces like \" \", \\n, \\t from the beginning and ending of text\n",
    "    \n",
    "    :param text - input text; it's a native python string\n",
    "    :return: the given text without spaces; \n",
    "    :rtype: built-in python string\n",
    "   \n",
    "    '''\n",
    "    return text.strip()\n",
    "\n",
    "print(remove_excessive_space(\"\\n\\n This is a text and this is another one \\n \\n \\t\"))\n",
    "\n",
    "# TO-DO: refactor the documentation\n",
    "def remove_punctuations(words):\n",
    "    '''\n",
    "    Remove all the punctuations from the given text\n",
    "    \n",
    "    :param text: the input text; it's a native python string\n",
    "    :param nlp_model: NLP model that is used to preprocess the text; it's a spacy.lang object\n",
    "    :return: the given text, without punctuations; \n",
    "    :rtype: built-in python string\n",
    "    '''\n",
    "    \n",
    "    words_res = []\n",
    "    for word in words:\n",
    "        if word.is_punct is False:\n",
    "            words_res.append(word)\n",
    "    \n",
    "    return words_res\n",
    "\n",
    "# TO-DO - refactor the description\n",
    "def remove_stopwords(words):\n",
    "    '''\n",
    "    Remove all the stop words from the given text\n",
    "    \n",
    "    :param text: the input text; it's a native python string\n",
    "    :param nlp_model: NLP model that is used to preprocess the text; it's a spacy.lang object\n",
    "    :return: the given text, without stop words; \n",
    "    :rtype: built-in python string\n",
    "    '''\n",
    "    words_res = []\n",
    "    for word in words:\n",
    "        if word.is_stop is False:\n",
    "            words_res.append(word)\n",
    "    \n",
    "    return words_res\n",
    "\n",
    "# TO-DO - refactor the description\n",
    "def lemmatize_words(words):\n",
    "    '''\n",
    "    Apply lemmatization for a list of words. \n",
    "    \n",
    "    :param words: the input list with words; every element is a spacy.tokens.token.Token object\n",
    "    :return: a list constructed from the initial one but every with is lemmatized (converted to base form)\n",
    "    :rtype: built-in python list, every element is a spacy.tokens.token.Token object\n",
    "    '''\n",
    "    \n",
    "    words_res = []\n",
    "    for word in words:\n",
    "        words_res.append(word.lemma_)\n",
    "    \n",
    "    return words_res\n",
    "\n",
    "\n",
    "# part of speech for every word\n",
    "def words_pos(words):\n",
    "    words_res = []\n",
    "    for word in words:\n",
    "        words_res.append( (word, word.pos_) ) \n",
    "    \n",
    "    return words_res\n",
    "\n",
    "def get_words_from_raw_text(text, nlp_model):\n",
    "    '''\n",
    "    Convert a raw text to a built-in python list of spacy.tokens.token.Token object (tokens); \n",
    "    \n",
    "    '''\n",
    "    doc = nlp_model(text)\n",
    "    words = []\n",
    "    for token in doc:\n",
    "        words.append(token)\n",
    "        \n",
    "    return words\n",
    "\n",
    "input_text = \"The quick brown foxes are jumping over the lazy dogs. They were running through the forests, exploring the mysterious caves. I saw many interesting books on the shelves and decided to read them all. The children were playing happily in the parks, swinging on the swings and climbing on the jungle gym. Despite the challenges, they were determined to succeed in their endeavors.\"\n",
    "words_input = get_words_from_raw_text(input_text, nlp_model)\n",
    "print(\"Tokens:\")\n",
    "print(words_input)\n",
    "print(\"-\" * 15)\n",
    "words_input = remove_punctuations(words_input)\n",
    "print(\"Without punctuations:\")\n",
    "print(words_input)\n",
    "print(\"-\" * 15)\n",
    "words_input = remove_stopwords(words_input)\n",
    "print(\"Without stopwords:\")\n",
    "print(words_input)\n",
    "print(\"-\" * 15)\n",
    "words_input = lemmatize_words(words_input)\n",
    "print(\"After lemmatization:\")\n",
    "print(words_input)\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# need to convert again to text and then to tokenize because the lemmatization convert words to built in string\n",
    "words_as_single_text = ' '.join(words_input)\n",
    "words_input = get_words_from_raw_text(words_as_single_text, nlp_model)\n",
    "words_and_pos = words_pos(words_input)\n",
    "print(\"Part of speech:\")\n",
    "print(words_and_pos)\n",
    "print(\"-\" * 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "777f2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IO functions\n",
    "def read_txt_file(file_path):\n",
    "    '''\n",
    "    Return the content from the file from the given path. We assume the first line is the document title and the\n",
    "    second line is document content\n",
    "    \n",
    "    :param file_path: path to the target file \n",
    "    :return: a dictionary with 2 entries: title and content of the file\n",
    "    :rtype: built-in python dictionary\n",
    "    '''\n",
    "    result = dict()\n",
    "    with open(file_path, 'r', encoding='utf-8') as file_obj:  \n",
    "                result['title'] = file_obj.readline()\n",
    "                result['content'] = file_obj.read()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db80ae32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lufthansa flies back to profit\\n</td>\n",
       "      <td>\\nGerman airline Lufthansa has returned to pro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Winn-Dixie files for bankruptcy\\n</td>\n",
       "      <td>\\nUS supermarket group Winn-Dixie has filed fo...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US economy still growing says Fed\\n</td>\n",
       "      <td>\\nMost areas of the US saw their economy conti...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saab to build Cadillacs in Sweden\\n</td>\n",
       "      <td>\\nGeneral Motors, the world's largest car make...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bank voted 8-1 for no rate change\\n</td>\n",
       "      <td>\\nThe decision to keep interest rates on hold ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Mobile games come of age\\n</td>\n",
       "      <td>\\nThe BBC News website takes a look at how gam...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>California sets fines for spyware\\n</td>\n",
       "      <td>\\nThe makers of computer programs that secretl...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Web helps collect aid donations\\n</td>\n",
       "      <td>\\nThe web is helping aid agencies gather resou...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Mobiles rack up 20 years of use\\n</td>\n",
       "      <td>\\nMobile phones in the UK are celebrating thei...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Blogs take on the mainstream\\n</td>\n",
       "      <td>\\nWeb logs or blogs are everywhere, with at le...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title  \\\n",
       "0       Lufthansa flies back to profit\\n   \n",
       "1      Winn-Dixie files for bankruptcy\\n   \n",
       "2    US economy still growing says Fed\\n   \n",
       "3    Saab to build Cadillacs in Sweden\\n   \n",
       "4    Bank voted 8-1 for no rate change\\n   \n",
       "..                                   ...   \n",
       "995           Mobile games come of age\\n   \n",
       "996  California sets fines for spyware\\n   \n",
       "997    Web helps collect aid donations\\n   \n",
       "998    Mobiles rack up 20 years of use\\n   \n",
       "999       Blogs take on the mainstream\\n   \n",
       "\n",
       "                                               content        type  \n",
       "0    \\nGerman airline Lufthansa has returned to pro...    business  \n",
       "1    \\nUS supermarket group Winn-Dixie has filed fo...    business  \n",
       "2    \\nMost areas of the US saw their economy conti...    business  \n",
       "3    \\nGeneral Motors, the world's largest car make...    business  \n",
       "4    \\nThe decision to keep interest rates on hold ...    business  \n",
       "..                                                 ...         ...  \n",
       "995  \\nThe BBC News website takes a look at how gam...  technology  \n",
       "996  \\nThe makers of computer programs that secretl...  technology  \n",
       "997  \\nThe web is helping aid agencies gather resou...  technology  \n",
       "998  \\nMobile phones in the UK are celebrating thei...  technology  \n",
       "999  \\nWeb logs or blogs are everywhere, with at le...  technology  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_raw_data(main_directory_path):\n",
    "    \" read all files from all directories from the given path;  return a pandas df with 3 columns: document title, content and type (label) \"\n",
    "    df = pd.DataFrame(columns=['title','content','type'])\n",
    "    directories = os.listdir(main_directory_path)\n",
    "    \n",
    "    new_files_contents = []\n",
    "    \n",
    "    for directory in directories:\n",
    "        directory_path = main_directory_path + \"\\\\\" + directory\n",
    "        files = os.listdir(directory_path)\n",
    "        for file in files:\n",
    "            file_path = directory_path + \"\\\\\" + file\n",
    "            file_content = read_txt_file(file_path)\n",
    "    \n",
    "            whole_file_content_as_dict = pd.DataFrame({'title':file_content['title'], 'content':file_content['content'], 'type':directory}, index = [0])\n",
    "            new_files_contents.append(whole_file_content_as_dict)\n",
    "                   \n",
    "    df = pd.concat([df] + new_files_contents, ignore_index=True)\n",
    "            \n",
    "    return df\n",
    "\n",
    "data_root_path = \"data\"\n",
    "df = read_raw_data(data_root_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8362353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load\n",
    "nlp_model = spacy.load(\"en_core_web_sm\")\n",
    "type(nlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b524ec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "German airline Lufthansa has returned to profit in 2004 after posting huge losses in 2003.\n",
      "\n",
      "\n",
      "In a preliminary report, the airline announced net profits of 400m euros ($527.61m; £274.73m), compared with a loss of 984m euros in 2003.\n",
      "Operating profits were at 380m euros, ten times more than in 2003.\n",
      "Lufthansa was hit in 2003 by tough competition and a dip in demand following the Iraq war and the killer SARS virus.\n",
      "It was also hit by troubles at its US catering business.\n",
      "Last year, Lufthansa showed signs of recovery even as some European and US airlines were teetering on the brink of bankruptcy.\n",
      "The board of Lufthansa has recommended paying a 2004 dividend of 0.30 euros per share.\n",
      "In 2003, shareholders did not get a dividend.\n",
      "The company said that it will give all the details of its 2004 results on 23 March.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_row = df.iloc[0]\n",
    "content = first_row['content']\n",
    "doc = nlp_model(content)\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fdc75f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
